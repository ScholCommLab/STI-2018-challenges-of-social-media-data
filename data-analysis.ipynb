{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STI 2018 - DOIs, URLs, and FB\n",
    "\n",
    "Code to produce quantification of three problem cases with the WOS state_of_oa dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "import itertools\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.1f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load resolved DOIs\n",
    "resolved_doi = pd.read_csv(\"data/wos_100k_resolved.csv\", index_col=\"doi\")\n",
    "resolved_doi['domain'] = resolved_doi.resolved.map(lambda x: urlparse(x)[1] if pd.notnull(x) else None)\n",
    "resolved_doi['prefix'] = resolved_doi.index.map(lambda x: x.split(\"/\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakdown of responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of articles: 103539\n",
      "Got response from crossref: 91490 (88.4%)\n",
      "Resolved with 200s: 85515 (82.6%)\n",
      "Resolved with error: 5975 (5.8%)\n",
      "RequestException + TimeOuts: 12049 (11.6%)\n",
      "Resolved to HTTPS: 69619\tHTTP: 21871\n",
      "68 (0.1%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06567573571311294"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = resolved_doi\n",
    "\n",
    "l = len(df)\n",
    "print(\"Total number of articles: {}\".format(l))\n",
    "\n",
    "a = df.status_code.notnull().sum()\n",
    "print(\"Got response from crossref: {} ({:.1f}%)\".format(a, a*100/l))\n",
    "a = df.status_code.value_counts()[200]\n",
    "print(\"Resolved with 200s: {} ({:.1f}%)\".format(a, a*100/l))\n",
    "a = len(df[df.status_code.notnull()]) - a\n",
    "print(\"Resolved with error: {} ({:.1f}%)\".format(a, a*100/l))\n",
    "a = df.err.value_counts()['RequestException'] + df.err.value_counts()['Timeout']\n",
    "print(\"RequestException + TimeOuts: {} ({:.1f}%)\".format(a, a*100/l))\n",
    "\n",
    "https_urls = df[df.resolved.notnull()].resolved.map(lambda x: x[4] == \"s\").sum()\n",
    "print(\"Resolved to HTTPS: {}\\tHTTP: {}\".format(https_urls, len(df[df.resolved.notnull()])-https_urls))\n",
    "\n",
    "dupl = df.resolved.duplicated(keep=False)\n",
    "notnull = df.resolved.notnull()\n",
    "print(\"{} ({:.1f}%)\".format(len(df[dupl&notnull]), len(df[dupl&notnull])/len(df[notnull])*100))\n",
    "len(df[dupl&notnull])/l*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many DOIs resolve to HTTP/HTTPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URL Variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big 5 DOIs: 55777 (53.9%)\n",
      "Samples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://linkinghub.elsevier.com/retrieve/pii/S1549963414003256',\n",
       " 'https://link.springer.com/article/10.1007%2Fs10337-010-1883-4',\n",
       " 'https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.909',\n",
       " 'https://www.tandfonline.com/doi/full/10.1179/1743282014Y.0000000119',\n",
       " 'http://journals.sagepub.com/doi/10.1177/1010539513486919']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_5 = ['linkinghub.elsevier.com',\n",
    "         'link.springer.com',\n",
    "         'onlinelibrary.wiley.com',\n",
    "         'www.tandfonline.com',\n",
    "         'journals.sagepub.com']\n",
    "\n",
    "ind = []\n",
    "for d in big_5:\n",
    "    ind.append(df[df.domain == d].sample().index[0])\n",
    "    \n",
    "a = len(df[df.domain.isin(big_5)])\n",
    "print(\"Big 5 DOIs: {} ({:.1f}%)\".format(a, 100*a/l))\n",
    "\n",
    "print(\"Samples\")\n",
    "df.loc[ind][['status_code', 'resolved', 'domain', 'prefix']].resolved.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consts\n",
    "ids = ['ogid'+str(i) for i in range (1,5)]\n",
    "eng = ['eng'+str(i) for i in range (1,5)]\n",
    "urls = ['url'+str(i) for i in range (1,5)]\n",
    "# shares = ['shares1','shares2','shares3','shares4']\n",
    "\n",
    "dtype={}\n",
    "for i in range(1,5):\n",
    "    dtype['url'+str(i)] = str\n",
    "    dtype['og_eng'+str(i)] = str\n",
    "    dtype['og_obj'+str(i)] = str\n",
    "    dtype['og_err'+str(i)] = str\n",
    "\n",
    "url_response = pd.read_csv(\"data/wos_100k_full.csv\", index_col=\"doi\", parse_dates=['ts'], dtype=dtype)\n",
    "\n",
    "# Prepare results\n",
    "results = url_response[urls].copy()\n",
    "for i in range(1,5):\n",
    "    results['ogid'+str(i)] = url_response['og_obj'+str(i)].map(lambda x: json.loads(x)['id'] if pd.notnull(x) else None)\n",
    "    results['eng'+str(i)] = url_response['og_eng'+str(i)].map(lambda x: sum(json.loads(x).values()) if pd.notnull(x) else None)\n",
    "    #results['shares'+str(i)] = df['og_eng'+str(i)].map(lambda x: json.loads(x)['share_count'] if pd.notnull(x) else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = results[eng].apply(lambda x: sum(x) > 0, axis=1)\n",
    "results_eng = results[x]\n",
    "\n",
    "x = results[ids].apply(lambda x: x.notnull().sum() > 0, axis=1)\n",
    "results_ids = results[x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTTP/HTTPS URL breakdown for articles with OG object or Eng>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENG</th>\n",
       "      <th>IDS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>http</th>\n",
       "      <td>3821</td>\n",
       "      <td>19901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https</th>\n",
       "      <td>684</td>\n",
       "      <td>1856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ENG    IDS\n",
       "http   3821  19901\n",
       "https   684   1856"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_https_breakdown(df):\n",
    "    http = 0\n",
    "    https = 0\n",
    "    df = df[['url1', 'url2','ogid1', 'ogid2']]\n",
    "    for row in df.itertuples():\n",
    "        if row[3]:\n",
    "            if row[1][4] == \"s\":\n",
    "                https = https + 1\n",
    "            else:\n",
    "                http = http + 1\n",
    "        if row[4]:\n",
    "            if row[2][4] == \"s\":\n",
    "                https = https + 1\n",
    "            else:\n",
    "                http = http + 1\n",
    "    return {'http':http, 'https':https}\n",
    "pd.DataFrame({'IDS':get_https_breakdown(results_ids),\n",
    "              'ENG':get_https_breakdown(results_eng)}, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage of 4 URL variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENG</th>\n",
       "      <th>ENG (%)</th>\n",
       "      <th>IDS</th>\n",
       "      <th>IDS (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1426</td>\n",
       "      <td>1.6</td>\n",
       "      <td>8452</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2458</td>\n",
       "      <td>2.7</td>\n",
       "      <td>13305</td>\n",
       "      <td>14.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>74</td>\n",
       "      <td>0.1</td>\n",
       "      <td>179</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2612</td>\n",
       "      <td>2.9</td>\n",
       "      <td>10124</td>\n",
       "      <td>11.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ENG  ENG (%)    IDS  IDS (%)\n",
       "0  1426      1.6   8452      9.2\n",
       "1  2458      2.7  13305     14.5\n",
       "2    74      0.1    179      0.2\n",
       "3  2612      2.9  10124     11.1"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov = results[ids].apply(lambda x: x.notnull().sum()).values\n",
    "cov_eng = results[eng].apply(lambda x: sum(x>0)).values\n",
    "pd.DataFrame({'IDS':cov,\n",
    "              'IDS (%)':cov/(len(results)/100),\n",
    "              'ENG':cov_eng,\n",
    "              'ENG (%)': cov_eng/(len(results)/100)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 - DOI shares spread across graph objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of queried articles: 91490\n",
      "Articles w/ at least 1 OG_ID:     26775 (29.27%)\n",
      "Articles w/ at least 2 OG_IDs:    5007 (5.47%)\n",
      "\tArticles with mismatching IDs (incl Null): 5006 (99.98%)\n",
      "\tArticles with mismatching IDs (excl Null): 1910 (38.15%)\n",
      "\tArticles with matching IDs: 3097 (61.85%)\n",
      "\t\tMatching IDs with mismatching shares (excl. 0): 69 (2.23%)\n",
      "\t\tMatching IDs with matching shares (excl. 0): 635 (20.50%)\n",
      "\t\tMatching IDs with shares: 2306 (74.46%)\n",
      "Items with engagement but no shares: 2306\n"
     ]
    }
   ],
   "source": [
    "tdf = results\n",
    "print(\"Total number of queried articles: {}\".format(len(tdf)))\n",
    "\n",
    "# At least one OG_ID\n",
    "min_one = tdf[ids].dropna(how=\"all\")\n",
    "min_two_bool = min_one.apply(lambda x: x.notnull().sum() > 1, axis=1)\n",
    "min_two = min_one[min_two_bool]\n",
    "\n",
    "# ============================\n",
    "\n",
    "l = len(tdf)\n",
    "a = len(min_one)\n",
    "print(\"Articles w/ at least 1 OG_ID:     {} ({:.2f}%)\".format(a, 100*a/l))\n",
    "a = len(min_two)\n",
    "print(\"Articles w/ at least 2 OG_IDs:    {} ({:.2f}%)\".format(a, 100*a/l))\n",
    "\n",
    "rows_with_mismatch_ids_null = min_two.apply(lambda x: len(set(x)) != 1, axis=1)\n",
    "rows_with_mismatch_ids = min_two.apply(lambda x: len(set([y for y in x if pd.notnull(y)])) != 1, axis=1)\n",
    "rows_with_match_ids = min_two.apply(lambda x: len(set([y for y in x if pd.notnull(y)])) == 1 and len([y for y in x if pd.notnull(y)])>1, axis=1)\n",
    "\n",
    "# ============================\n",
    "\n",
    "l = len(min_two)\n",
    "a = rows_with_mismatch_ids_null.sum()\n",
    "print(\"\\tArticles with mismatching IDs (incl Null): {} ({:.2f}%)\".format(a, 100*a/l))\n",
    "a = rows_with_mismatch_ids.sum()\n",
    "print(\"\\tArticles with mismatching IDs (excl Null): {} ({:.2f}%)\".format(a, 100*a/l))\n",
    "a = rows_with_match_ids.sum()\n",
    "print(\"\\tArticles with matching IDs: {} ({:.2f}%)\".format(a, 100*a/l))\n",
    "\n",
    "# ============================\n",
    "\n",
    "res_ogids_match = results.loc[min_two[rows_with_match_ids].index]\n",
    "mismatch_shares = res_ogids_match[shares].apply(lambda x: len(set([y for y in x if y!=0])) > 1, axis=1)\n",
    "match_shares = res_ogids_match[shares].apply(lambda x: len(set([y for y in x if y!=0])) == 1 and len([y for y in x if y!=0])>1, axis=1)\n",
    "has_shares = res_ogids_match[shares].apply(lambda x: len(set([y for y in x if y!=0])) < 1, axis=1)\n",
    "\n",
    "l = len(res_ogids_match)\n",
    "a = mismatch_shares.sum()\n",
    "print(\"\\t\\tMatching IDs with mismatching shares (excl. 0): {} ({:.2f}%)\".format(a, 100*a/l))\n",
    "a = match_shares.sum()\n",
    "print(\"\\t\\tMatching IDs with matching shares (excl. 0): {} ({:.2f}%)\".format(a, 100*a/l))\n",
    "a = has_shares.sum()\n",
    "print(\"\\t\\tMatching IDs with no shares: {} ({:.2f}%)\".format(a, 100*a/l))\n",
    "print(\"Items with engagement but no shares: {}\".format(len(res_ogids_match[has_shares])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of queried articles WITH ENG > 0: 5498\n",
      "Total at least one OG_ID: 5392\n",
      "One OG_ID    3687\n",
      "Two OG_ID    1535\n",
      "Three OG_ID  161\n",
      "Four OG_ID   9\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of queried articles WITH ENG > 0: {}\".format(len(results_eng)))\n",
    "\n",
    "# ============================\n",
    "tdf = results_eng[ids].dropna(how=\"all\")\n",
    "print(\"Total at least one OG_ID: {}\".format(len(tdf)))\n",
    "\n",
    "# One OG_ID\n",
    "x = tdf.apply(lambda x: x.notnull().sum() == 1, axis=1)\n",
    "one = tdf[x]\n",
    "print(\"One OG_ID    {}\".format(len(one)))\n",
    "\n",
    "# One OG_ID\n",
    "x = tdf.apply(lambda x: x.notnull().sum() == 2, axis=1)\n",
    "two = tdf[x]\n",
    "print(\"Two OG_ID    {}\".format(len(two)))\n",
    "\n",
    "# One OG_ID\n",
    "x = tdf.apply(lambda x: x.notnull().sum() == 3, axis=1)\n",
    "three = tdf[x]\n",
    "print(\"Three OG_ID  {}\".format(len(three)))\n",
    "\n",
    "# One OG_ID\n",
    "x = tdf.apply(lambda x: x.notnull().sum() == 4, axis=1)\n",
    "four = tdf[x]\n",
    "print(\"Four OG_ID   {}\".format(len(four)))\n",
    "\n",
    "# ============================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pairs(row):\n",
    "    ids = ['1', '2', '3', '4']\n",
    "    \n",
    "    atleastonematching = False\n",
    "    for c in itertools.combinations(ids, 2):\n",
    "        x = c[0]\n",
    "        y = c[1]\n",
    "        \n",
    "        # if one of the Ob_IDs is empty, move on\n",
    "        if row['og_id' + x] is None or row['og_id' + y] is None:\n",
    "            continue\n",
    "\n",
    "        # keep track of matching Ob_IDs\n",
    "        if row['og_id' + x] == row['og_id' + y]:\n",
    "            atleastonematching = True\n",
    "\n",
    "            # if we have matching IDs, but non matching values, it is a problem\n",
    "            if row['eng' + x] != row['eng' + y]:\n",
    "                return False\n",
    "    \n",
    "    if atleastonematching:\n",
    "        return True\n",
    "\n",
    "    return \"nonmatch\"\n",
    "\n",
    "def check_nonmatching(row):\n",
    "    ids = ['1', '2', '3', '4']\n",
    "    \n",
    "    for c in itertools.combinations(ids, 2):\n",
    "        x = c[0]\n",
    "        y = c[1]\n",
    "        \n",
    "        # if one of the Ob_IDs is empty, move on\n",
    "        if row['og_id' + x] is None or row['og_id' + y] is None:\n",
    "            continue\n",
    "        \n",
    "        if row['og_id' + x] != row['og_id' + y]:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "check_pairs\n",
      "False       146\n",
      "True        620\n",
      "nonmatch    769\n",
      "dtype: int64\n",
      "nonmatch 769\n"
     ]
    }
   ],
   "source": [
    "df = two_eng\n",
    "\n",
    "df['check_pairs'] = df.apply(check_pairs, axis=1)\n",
    "x = df.groupby('check_pairs').size()\n",
    "print(x)\n",
    "print(\"nonmatch\", df.apply(check_nonmatching, axis=1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "check_pairs\n",
      "False       43\n",
      "True        99\n",
      "nonmatch    19\n",
      "dtype: int64\n",
      "nonmatch 131\n"
     ]
    }
   ],
   "source": [
    "df = three_eng\n",
    "\n",
    "df['check_pairs'] = df.apply(check_pairs, axis=1)\n",
    "x = df.groupby('check_pairs').size()\n",
    "print(x)\n",
    "print(\"nonmatch\", df.apply(check_nonmatching, axis=1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "check_pairs\n",
      "False    3\n",
      "True     6\n",
      "dtype: int64\n",
      "nonmatch 8\n"
     ]
    }
   ],
   "source": [
    "df = four_eng\n",
    "\n",
    "df['check_pairs'] = df.apply(check_pairs, axis=1)\n",
    "x = df.groupby('check_pairs').size()\n",
    "print(len(df) == x.sum())\n",
    "print(x)\n",
    "\n",
    "print(\"nonmatch\", df.apply(check_nonmatching, axis=1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not matching IDs        19\n",
      "Matching IDs, NM Eng    31\n",
      "Matching IDs, M Eng     81\n",
      "Check if SUM == TWO:    False\n"
     ]
    }
   ],
   "source": [
    "three_eng = results.loc[three.index]\n",
    "four_eng = results.loc[four.index]\n",
    "\n",
    "nm_ids = three_eng[ids].apply(lambda x: len(set([y for y in x if pd.notnull(y)])) == 3, axis=1)\n",
    "m_ids_m_eng = []\n",
    "m_ids_nm_eng = []\n",
    "\n",
    "matching = three_eng[ids].apply(lambda x: len(set([y for y in x if pd.notnull(y)])) == 2, axis=1)\n",
    "for i, row in three_eng[matching][ids+eng].iterrows():\n",
    "    y = row[ids].tolist()\n",
    "    d = [x for x in y if y.count(x) > 1][0]\n",
    "    indices = [i for i,x in enumerate(y) if x == d]\n",
    "    if len(set(row[eng][['eng'+str(i+1) for i in indices]]))==1:\n",
    "        m_ids_m_eng.append(True)\n",
    "    else:\n",
    "        m_ids_m_eng.append(False)\n",
    "\n",
    "matching = three_eng[ids].apply(lambda x: len(set([y for y in x if pd.notnull(y)])) == 1, axis=1)\n",
    "for i, row in three_eng[matching][ids+eng].iterrows():\n",
    "    y = row[ids].tolist()\n",
    "    d = [x for x in y if y.count(x) > 1][0]\n",
    "    indices = [i for i,x in enumerate(y) if x == d]\n",
    "    if len(set(row[eng][['eng'+str(i+1) for i in indices]]))==1:\n",
    "        m_ids_m_eng.append(True)\n",
    "    else:\n",
    "        m_ids_m_eng.append(False)    \n",
    "m_ids_nm_eng = [not x for x in m_ids_m_eng]\n",
    "\n",
    "\n",
    "\n",
    "print(\"Not matching IDs        {}\".format(nm_ids.sum()))\n",
    "print(\"Matching IDs, NM Eng    {}\".format(sum(m_ids_nm_eng)))\n",
    "print(\"Matching IDs, M Eng     {}\".format(sum(m_ids_m_eng)))\n",
    "print(\"Check if SUM == TWO:    {}\".format(nm_ids.sum()+sum(m_ids_m_eng)+sum(m_ids_nm_eng) == len(three)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = results[eng].apply(lambda x: sum(x) > 0, axis=1)\n",
    "tdf = results[x]\n",
    "print(\"Total number of queried articles WITH ENG > 0: {}\".format(len(tdf)))\n",
    "\n",
    "# ============================\n",
    "tdf = tdf[ids].dropna(how=\"all\")\n",
    "\n",
    "# One OG_ID\n",
    "x = tdf.apply(lambda x: x.notnull().sum() == 1, axis=1)\n",
    "one = tdf[x]\n",
    "\n",
    "l = len(tdf)\n",
    "a = len(min_one)\n",
    "print(\"Articles w/ at least 1 OG_ID:     {} ({:.2f}%)\".format(a, 100*a/l))\n",
    "a = len(min_two)\n",
    "print(\"Articles w/ at least 2 OG_IDs:    {} ({:.2f}%)\".format(a, 100*a/l))\n",
    "\n",
    "# ============================\n",
    "\n",
    "rows_with_mismatch_ids_null = min_two.apply(lambda x: len(set(x)) != 1, axis=1)\n",
    "rows_with_mismatch_ids = min_two.apply(lambda x: len(set([y for y in x if pd.notnull(y)])) != 1, axis=1)\n",
    "rows_with_match_ids = min_two.apply(lambda x: len(set([y for y in x if pd.notnull(y)])) == 1 and len([y for y in x if pd.notnull(y)])>1, axis=1)\n",
    "\n",
    "l = len(min_two)\n",
    "a = rows_with_mismatch_ids_null.sum()\n",
    "print(\"\\tArticles with mismatching IDs (incl Null): {} ({:.2f}%)\".format(a, 100*a/l))\n",
    "a = rows_with_mismatch_ids.sum()\n",
    "print(\"\\tArticles with mismatching IDs (excl Null): {} ({:.2f}%)\".format(a, 100*a/l))\n",
    "a = rows_with_match_ids.sum()\n",
    "print(\"\\tArticles with matching IDs: {} ({:.2f}%)\".format(a, 100*a/l))\n",
    "\n",
    "# ============================\n",
    "\n",
    "res_ogids_match = results.loc[min_two[rows_with_match_ids].index]\n",
    "mismatch_shares = res_ogids_match[eng].apply(lambda x: len(set([y for y in x if y!=0])) > 1, axis=1)\n",
    "match_shares = res_ogids_match[eng].apply(lambda x: len(set([y for y in x if y!=0])) == 1 and len([y for y in x if y!=0])>1, axis=1)\n",
    "has_shares = res_ogids_match[eng].apply(lambda x: len(set([y for y in x if y!=0])) < 1, axis=1)\n",
    "\n",
    "l = len(res_ogids_match)\n",
    "a = mismatch_shares.sum()\n",
    "print(\"\\t\\tMatching IDs with mismatching shares (excl. 0): {} ({:.2f}%)\".format(a, 100*a/l))\n",
    "a = match_shares.sum()\n",
    "print(\"\\t\\tMatching IDs with matching shares (excl. 0): {} ({:.2f}%)\".format(a, 100*a/l))\n",
    "a = has_shares.sum()\n",
    "print(\"\\t\\tMatching IDs with no shares: {} ({:.2f}%)\".format(a, 100*a/l))\n",
    "print(\"Items with engagement but no shares: {}\".format(len(res_ogids_match[has_shares])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of queried articles WITH ENG > 0: 5498\n",
      "Articles w/ at least 1 OG_ID:     5392 (98.07%)\n",
      "Articles w/ at least 2 OG_IDs:    1705 (31.01%)\n",
      "\tArticles with mismatching IDs (incl Null): 1704 (99.94%)\n",
      "\tArticles with mismatching IDs (excl Null): 908 (53.26%)\n",
      "\tArticles with matching IDs: 797 (46.74%)\n",
      "\t\tMatching IDs with mismatching shares (excl. 0): 72 (9.03%)\n",
      "\t\tMatching IDs with matching shares (excl. 0): 638 (80.05%)\n",
      "\t\tMatching IDs with no shares: 0 (0.00%)\n",
      "Items with engagement but no shares: 0\n"
     ]
    }
   ],
   "source": [
    "tdf = results[eng].apply(lambda x: sum(x) > 0, axis=1)\n",
    "tdf = results[tdf]\n",
    "print(\"Total number of queried articles WITH ENG > 0: {}\".format(len(tdf)))\n",
    "\n",
    "# ============================\n",
    "\n",
    "# At least one OG_ID\n",
    "min_one = tdf[ids].dropna(how=\"all\")\n",
    "min_two_bool = min_one.apply(lambda x: x.notnull().sum() > 1, axis=1)\n",
    "min_two = min_one[min_two_bool]\n",
    "\n",
    "l = len(tdf)\n",
    "a = len(min_one)\n",
    "print(\"Articles w/ at least 1 OG_ID:     {} ({:.2f}%)\".format(a, 100*a/l))\n",
    "a = len(min_two)\n",
    "print(\"Articles w/ at least 2 OG_IDs:    {} ({:.2f}%)\".format(a, 100*a/l))\n",
    "\n",
    "# ============================\n",
    "\n",
    "rows_with_mismatch_ids_null = min_two.apply(lambda x: len(set(x)) != 1, axis=1)\n",
    "rows_with_mismatch_ids = min_two.apply(lambda x: len(set([y for y in x if pd.notnull(y)])) != 1, axis=1)\n",
    "rows_with_match_ids = min_two.apply(lambda x: len(set([y for y in x if pd.notnull(y)])) == 1 and len([y for y in x if pd.notnull(y)])>1, axis=1)\n",
    "\n",
    "l = len(min_two)\n",
    "a = rows_with_mismatch_ids_null.sum()\n",
    "print(\"\\tArticles with mismatching IDs (incl Null): {} ({:.2f}%)\".format(a, 100*a/l))\n",
    "a = rows_with_mismatch_ids.sum()\n",
    "print(\"\\tArticles with mismatching IDs (excl Null): {} ({:.2f}%)\".format(a, 100*a/l))\n",
    "a = rows_with_match_ids.sum()\n",
    "print(\"\\tArticles with matching IDs: {} ({:.2f}%)\".format(a, 100*a/l))\n",
    "\n",
    "# ============================\n",
    "\n",
    "res_ogids_match = results.loc[min_two[rows_with_match_ids].index]\n",
    "mismatch_shares = res_ogids_match[eng].apply(lambda x: len(set([y for y in x if y!=0])) > 1, axis=1)\n",
    "match_shares = res_ogids_match[eng].apply(lambda x: len(set([y for y in x if y!=0])) == 1 and len([y for y in x if y!=0])>1, axis=1)\n",
    "has_shares = res_ogids_match[eng].apply(lambda x: len(set([y for y in x if y!=0])) < 1, axis=1)\n",
    "\n",
    "l = len(res_ogids_match)\n",
    "a = mismatch_shares.sum()\n",
    "print(\"\\t\\tMatching IDs with mismatching shares (excl. 0): {} ({:.2f}%)\".format(a, 100*a/l))\n",
    "a = match_shares.sum()\n",
    "print(\"\\t\\tMatching IDs with matching shares (excl. 0): {} ({:.2f}%)\".format(a, 100*a/l))\n",
    "a = has_shares.sum()\n",
    "print(\"\\t\\tMatching IDs with no shares: {} ({:.2f}%)\".format(a, 100*a/l))\n",
    "print(\"Items with engagement but no shares: {}\".format(len(res_ogids_match[has_shares])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 - URLs collapse into same graph object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asura/.virtualenvs/altmetrics/lib/python3.5/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate ids: 66\n",
      "Number of articles affected: 507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asura/.virtualenvs/altmetrics/lib/python3.5/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "df = results[ids]\n",
    "ids = ['og_id%s' % i for i in range(1,5)]\n",
    "\n",
    "df['all_ids'] = df[ids].apply(lambda x: [int(y) for y in set(x) if pd.notnull(y)], axis=1)\n",
    "\n",
    "all_ids = df.all_ids.sum()\n",
    "\n",
    "counter = collections.Counter(all_ids)\n",
    "dup_ids = set([i for (i,v) in counter.items() if v > 1])\n",
    "\n",
    "print(\"Number of duplicate ids: %s\" % len(dup_ids))\n",
    "\n",
    "df['has_dup'] = df.all_ids.map(lambda x: len(dup_ids.intersection(x)) > 0)\n",
    "print(\"Number of articles affected: %s\" % df.has_dup.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91490/91490 [00:02<00:00, 31565.48it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = results[ids].reset_index(drop=True)\n",
    "\n",
    "dupls = [False] * len(df)\n",
    "seen_ids = set()\n",
    "x = {}\n",
    "\n",
    "arr = df.values\n",
    "a = arr[arr != np.array(None)].ravel()\n",
    "q = np.unique(a, return_counts=True)\n",
    "dup_ids = q[0][q[1] > 1]\n",
    "\n",
    "f = lambda x: set(np.where(arr == x)[0])\n",
    "fv = np.vectorize(f)\n",
    "c = fv(dup_ids)\n",
    "    \n",
    "seen_ids = set()\n",
    "for row in tqdm(df.itertuples(), total=len(df)):\n",
    "    for val in [x for x in set(row[1:5]) if x is not None]:\n",
    "        if val not in seen_ids:\n",
    "            try:\n",
    "                indices = c[dup_ids==val]\n",
    "                if len(indices)>1:\n",
    "                    seen_ids.add(val)\n",
    "                    for i in indices:\n",
    "                        dupls[i] = True\n",
    "            except:\n",
    "                pass\n",
    "sum(dupls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1649721495056000</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1382544005159501</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>926022794172969</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1540865556012732</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10150284009700488</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>947150088698533</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1827602397257489</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>566483340123620</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>456681777755197</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1826664834073265</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1688337074541082</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1577953665618451</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1568262766555299</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1075089142531341</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                index     0\n",
       "0    1649721495056000  38.0\n",
       "1    1382544005159501  10.0\n",
       "2     926022794172969   8.0\n",
       "3    1540865556012732   4.0\n",
       "4   10150284009700488   4.0\n",
       "5     947150088698533   3.0\n",
       "6    1827602397257489   3.0\n",
       "7     566483340123620   2.0\n",
       "8     456681777755197   2.0\n",
       "9    1826664834073265   2.0\n",
       "10   1688337074541082   2.0\n",
       "11   1577953665618451   2.0\n",
       "12   1568262766555299   2.0\n",
       "13   1075089142531341   2.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = df[dupls].apply(pd.value_counts).sum(axis=1)\n",
    "x[x>1].sort_values(ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interesting DOIs\n",
    "\n",
    "**10.1007/s00586-013-2675-y**\n",
    "\n",
    "Same OG ID for http/https but different one for DOI. Different share numbers\n",
    "\n",
    "**10.1038/nature13893**\n",
    "\n",
    "Different OG objects\n",
    "\n",
    "**10.7717/peerj.794**\n",
    "\n",
    "Same share numbers, different OG IDs\n",
    "\n",
    "**10.1016/j.aap.2014.03.007**\n",
    "\n",
    "Elsevier redirect page, various OG IDs\n",
    "\n",
    "**10.7440/res53.2015.10**\n",
    "\n",
    "Various IDs across DOI, URL\n",
    "\n",
    "## Used code\n",
    "\n",
    "```\n",
    "doi = '10.7440/res53.2015.10'\n",
    "rec = df.loc[doi]\n",
    "urls = [rec.url, rec.url2, \"https://doi.org/%s\" % doi, \"http://dx.doi.org/%s\" % doi]\n",
    "pprint(fb_queries(urls))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem with missing scraped data\n",
    "\n",
    "Results for the http and https for journals.ametsoc.org/doi/abs/10.1175/JAS-D-12-0315.1 \n",
    "\n",
    "URL    | OG ID            | Shares | Date (scrape)\n",
    "-------|------------------|--------|---------------\n",
    "HTTP   | 685490234864647  | 2      | September 30, 2016\n",
    "HTTPS  | None             | None   | None\n",
    "\n",
    "After manually triggering a rescrape:\n",
    "\n",
    "URL    | OG ID            | Shares | Date (scrape)\n",
    "-------|------------------|--------|---------------\n",
    "HTTP   | 1818366768210382 | 0      | March 28, 2018\n",
    "HTTPS  | 1818366768210382 | 0      | March 28, 2018\n",
    "\n",
    "Apparently shares associated with previous canonical URLs are lost..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "altmetrics",
   "language": "python",
   "name": "altmetrics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
